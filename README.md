# üçΩÔ∏è Food Image Classification Using CNN & Transfer Learning

## **1. Overview**

This project develops and compares four deep learning models for classifying 20 food categories from a curated subset of Food-101:

1. Vanilla CNN (no augmentation)
2. CNN with data augmentation
3. Transfer Learning using Xception
4. Transfer Learning + Augmentation

The objective is to understand how data augmentation and transfer learning affect performance, training efficiency, and generalization.

## **2. Dataset**

* Source: Food-101 (20 selected classes, 1,000 images/class)
* Total images: 20,000
* Split:

  * Train: 16,000
  * Validation: 2,000
  * Test: 2,000
* Image sizes:

  * CNN: 224√ó224√ó3
  * Xception: 299√ó299√ó3

Dataset description from the ppt.

## **3. Model Summary**

### **3.1 Vanilla CNN**

* 4 convolutional blocks (32‚Üí64‚Üí128‚Üí256 filters)
* BatchNorm + MaxPooling + Dropout
* GlobalAveragePooling + Dense(256‚Üí128) + Softmax(20)
* Optimizer: AdamW
* Regularization: L2, Label Smoothing, Mixed Precision

### **3.2 CNN + Data Augmentation**

* Augmentations: rotation, shear, zoom, shift, brightness, color jitter, flips
* 4 deeper blocks (64‚Üí512 filters, 3 conv/layer)
* One-cycle LR schedule, AdamW

### **3.3 Xception (Transfer Learning)**

* Pretrained on ImageNet
* Two-stage training:

  * Feature Extraction ‚Üí train custom head only
  * Fine-tuning ‚Üí unfreeze top ~100 layers
* Dense head: GAP ‚Üí 512 ‚Üí 256 ‚Üí Softmax

### **3.4 Xception + Augmentation**

* Same augmentation as 3.2
* Parameter summary:

  * Total: ~22M
  * Trainable: ~1.19M
  * Non-trainable: ~20.86M

Figures and architecture details from pages 20‚Äì33 of the report .

## **4. Results**

| Model          | Acc        | Prec       | Recall     | F1         |
| -------------- | ---------- | ---------- | ---------- | ---------- |
| CNN            | 0.8135     | 0.8163     | 0.8135     | 0.8123     |
| CNN + Aug      | 0.8450     | 0.8525     | 0.8450     | 0.8458     |
| Xception       | 0.8780     | 0.8806     | 0.8780     | 0.8784     |
| Xception + Aug | **0.9025** | **0.9043** | **0.9025** | **0.9029** |

**Key takeaway:** Transfer learning provides the largest performance boost, while augmentation further strengthens generalization.

## **5. Conclusion**

* Pretrained Xception significantly outperforms CNN trained from scratch.
* Data augmentation reduces overfitting and improves robustness.
* The best configuration: **Transfer Learning + Augmentation**.
* Suitable for food recognition applications such as nutrition estimation, menu digitization, and culinary analytics.

# üáªüá≥ **(VI)**

## **1. T·ªïng quan**

D·ª± √°n x√¢y d·ª±ng v√† so s√°nh b·ªën m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh m√≥n ƒÉn (20 l·ªõp) t·ª´ t·∫≠p Food-101:

1. CNN c∆° b·∫£n
2. CNN + tƒÉng c∆∞·ªùng d·ªØ li·ªáu
3. Transfer Learning v·ªõi Xception
4. Transfer Learning + Augmentation

M·ª•c ti√™u l√† ƒë√°nh gi√° t√°c ƒë·ªông c·ªßa augmentation v√† transfer learning l√™n hi·ªáu su·∫•t m√¥ h√¨nh.

## **2. B·ªô d·ªØ li·ªáu**

* Ngu·ªìn: Food-101 (20 l·ªõp, 1.000 ·∫£nh m·ªói l·ªõp)
* T·ªïng s·ªë ·∫£nh: 20.000
* Chia t·∫≠p:

  * Train: 16.000
  * Val: 2.000
  * Test: 2.000
* K√≠ch th∆∞·ªõc ·∫£nh: 224√ó224√ó3 ho·∫∑c 299√ó299√ó3

## **3. M√¥ h√¨nh**

### **3.1 CNN c∆° b·∫£n**

* 4 block t√≠ch ch·∫≠p (32‚Üí256)
* BatchNorm + Pooling + Dropout
* GAP ‚Üí Dense(256‚Üí128) ‚Üí Softmax(20)
* AdamW, L2, Label Smoothing

### **3.2 CNN + Augmentation**

* Xoay, d·ªãch, shear, zoom, tƒÉng s√°ng, bi·∫øn ƒë·ªïi m√†u, l·∫≠t
* Ki·∫øn tr√∫c s√¢u h∆°n (64‚Üí512)

### **3.3 Xception (Transfer Learning)**

* Pretrained ImageNet
* Hu·∫•n luy·ªán 2 giai ƒëo·∫°n
* Head: GAP ‚Üí 512 ‚Üí 256 ‚Üí Softmax

### **3.4 Xception + Augmentation**

* ~22M tham s·ªë, trong ƒë√≥ trainable ~1.19M

## **4. K·∫øt qu·∫£**

| Model          | Acc        | Prec       | Recall     | F1         |
| -------------- | ---------- | ---------- | ---------- | ---------- |
| CNN            | 0.8135     | 0.8163     | 0.8135     | 0.8123     |
| CNN + Aug      | 0.8450     | 0.8525     | 0.8450     | 0.8458     |
| Xception       | 0.8780     | 0.8806     | 0.8780     | 0.8784     |
| Xception + Aug | **0.9025** | **0.9043** | **0.9025** | **0.9029** |

**K·∫øt lu·∫≠n ch√≠nh:** Transfer Learning c·∫£i thi·ªán m·∫°nh nh·∫•t; augmentation tƒÉng kh·∫£ nƒÉng kh√°i qu√°t.

## **5. K·∫øt lu·∫≠n**

* Xception v∆∞·ª£t tr·ªôi so v·ªõi CNN t·ª± hu·∫•n luy·ªán.
* Augmentation gi·∫£m overfitting.
* M√¥ h√¨nh t·ªët nh·∫•t: **Xception + Augmentation**.
* Ph√π h·ª£p cho ·ª©ng d·ª•ng nh·∫≠n di·ªán m√≥n ƒÉn, g·ª£i √Ω dinh d∆∞·ª°ng, ph√¢n t√≠ch h√¨nh ·∫£nh ·∫©m th·ª±c.
